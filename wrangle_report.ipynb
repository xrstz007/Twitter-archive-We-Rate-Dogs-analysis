{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a098f0fe",
   "metadata": {},
   "source": [
    "## Wrangle Report\n",
    "\n",
    "For the data wrangling project on Udacity Data Analyst Nanodegree, the learners are given the chance to go through the whole data analysis process with more emphasis on data wrangling, from collecting the data to cleaning and analyzing it and finally visualizing trends from the data. The data collected was from the Twitter account ‘WeRateDogs’, a humorous account which gives most dogs a rating above 10.\n",
    "\n",
    "According to wikipedia, Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d8672",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "The twitter_archive_enhanced.csv file was given as a resource for this project. This was read into the jupyter notebook using the pandas.read function. \n",
    "\n",
    "The tweet image file which contains a prediction model for the images for each tweet was also provided but had to be programmatically downloaded. This file (image_predictions.tsv) is present in each tweet according to a neural network. It is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "The last source of data is Twitter. Web scapping off Twitter using its Tweepy API to get tweets archive using the tweet ids that were provided in the twitter_archive_enhanced file. Web scapping using APIs had been taught extensively on the Udacity nanodegree. The documentation for Tweepy can be found using this link, https://docs.tweepy.org/en/stable/api.html. The json file downloaded was read into a dataframe, tweets using pandas.\n",
    "\n",
    "The gathering process provided three data files to work with;\n",
    "twitter_archive\n",
    "image_prediction\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf7dcc",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "Of course, since there were three different data sources, there had to be problems between the three files. The task at hand was to find and clean at least 8 data quality and tidiness issues. I managed to find and clean 14 issues; 12 data quality issues and 2 data tidiness issues.\n",
    "\n",
    "This part of the project required the most time. Using the assess, code and test model, all the issues were carefully assessed. The required cleaning processes were carried out programmatically and they were adequately tested.\n",
    "\n",
    "My approach to finding these data issues was to first find basic information about the three data sets. Then I did a visual analysis i.e. I just looked at specific columns of the data and found issues, such as there being a lot of missing values in a few columns which were then validated via programmatic analysis such as using the info() function.\n",
    "\n",
    "The issues I found were major such as missing values, bad data types, bad data within the entries and smaller issues such as dog stages in 4 different columns which can be fit into one column and the different number of records within the three data files including duplication and retweets which had to be removed.\n",
    "\n",
    "The final list of issues found can be divided into two main types:\n",
    "\n",
    "Quality Issues:- Here, the issues are because of dirty data i.e. the data has problems with its content. Common data quality issues include missing data, invalid data, inaccurate data, and inconsistent data.\n",
    "Tidiness Issues:- Basically, here, issues are due to the structure of the data. It can also be referred to as messy data. A good guide to tidy and untidy data is found here.\n",
    "\n",
    "Once the data cleaning process was carried out, two master data sets were saved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
